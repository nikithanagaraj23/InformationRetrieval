{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(result):\n",
    "    doc   = open(\"test-collection/cacm/\"+ result + \".html\",\"r\")\n",
    "    data = doc.read()\n",
    "    data = BeautifulSoup(data, \"lxml\").text\n",
    "    data = casefolding(data)\n",
    "    sentences  = re.split(r\"\\.[\\s\\n]+ | \\n\",data)\n",
    "    sentences = [s.replace(\"\\n\",\" \").replace(\"\\t\",\" \") for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocWordFreq(word,document):\n",
    "    \n",
    "    #print(unigram_invertedlist_count.get(word))\n",
    "    value = 0\n",
    "    if unigram_invertedlist_count.get(word):\n",
    "        for val in unigram_invertedlist_count[word][1]:\n",
    "            if(val[0]==document):\n",
    "                value = val[1]\n",
    "    return value\n",
    "\n",
    "\n",
    "def calculate_significant_words(sentences, document, \\\n",
    "                                query_words, inverted_index):\n",
    "    \n",
    "    significant_words = []\n",
    "    words             = []\n",
    "    sd                = len(sentences)\n",
    "    \n",
    "    if (sd < 25): thresh = 4 - 0.1 * (25 - sd)\n",
    "    \n",
    "    elif ((25 <= sd) and (sd <= 40)): thresh = 4\n",
    "    \n",
    "    else: thresh = 4 + 0.1 * (sd - 40)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words.extend(word_tokenize(sentence))\n",
    "    \n",
    "    for word in words:   \n",
    "        f_dw = getDocWordFreq(word, document)\n",
    "        if f_dw >= thresh: significant_words.append(word)\n",
    "            \n",
    "    significant_words.extend(query_words)\n",
    "    significant_words = list(set(significant_words))\n",
    "\n",
    "\n",
    "    return significant_words      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_significance_factor(sentences, significant_words):\n",
    "    \n",
    "    word_count = 0\n",
    "    significance_factor = {}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence          = word_tokenize(sentence)\n",
    "        filtered_significance_words = set(tokenized_sentence)\\\n",
    "                                        .intersection(set(significant_words))\n",
    "        min_index = 100000;\n",
    "        max_index = -1;\n",
    "        \n",
    "        filtered_significance_words = [x for x in filtered_significance_words if x not in commonwords]\n",
    "    \n",
    "        for token in tokenized_sentence:\n",
    "            #print(\"token\",token)\n",
    "            if token in filtered_significance_words: \n",
    "                \n",
    "                new_max_index = max([i for i, x in enumerate(tokenized_sentence) if x == token])\n",
    "                new_min_index = min([i for i, x in enumerate(tokenized_sentence) if x == token])\n",
    "                \n",
    "                if(min_index > new_min_index):\n",
    "                    min_index = new_min_index\n",
    "                \n",
    "                if(max_index < new_max_index):\n",
    "                    max_index = new_max_index\n",
    "                \n",
    "                text_span = (max_index - min_index) + 0.0001\n",
    "        \n",
    "                count = sum([1 for w in tokenized_sentence[min_index: max_index] if w in filtered_significance_words])\n",
    "                significance_factor[sentence] = count**2/text_span\n",
    "    \n",
    "    return significance_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "significance_factor = calculate_significance_factor(sentences, significant_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_summary(results, query):\n",
    "    \n",
    "    with open ('unigram_invertedlist_count.pkl', 'rb') as f: \n",
    "        unigram_invertedlist_count = pickle.load(f)\n",
    "    \n",
    "    with open('inverted_index.pkl', 'rb') as f:\n",
    "        inverted_index = pickle.load(f)\n",
    "    \n",
    "    commonwords     = open('test-collection/common_words', \"r\")\n",
    "    commonwords     = commonwords.read().split('\\n')\n",
    "    query_words     = query.split(' ')\n",
    "    query_words     = [x for x in query_words if x not in commonwords]\n",
    "    significance_df = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    \n",
    "    for result in results:\n",
    "        significance_dict = []\n",
    "        sentences         = get_sentences(result)\n",
    "        significant_words = calculate_significant_words(sentences, \n",
    "                                                        result, query_words, \n",
    "                                                        inverted_index)\n",
    "        \n",
    "        for sent, score in calculate_significance_factor(sentences, significant_words).items():\n",
    "            \n",
    "            for query_word in query_words:\n",
    "                \n",
    "                sent = sent.replace(query_word, '<b> ' + query_word + '</b>')\n",
    "        \n",
    "            significance_dict.append({'result' : result, \n",
    "                                        'significance_factor': score,\n",
    "                                         'sentence': sent})\n",
    "        #print('\\n\\n\\n')\n",
    "        #print(pd.DataFrame.from_dict(significance_dict).sort_values('significance_factor', ascending=False)[:3])\n",
    "        \n",
    "        significance_df = significance_df.append(pd.DataFrame.from_dict(significance_dict)\\\n",
    "                                    .sort_values('significance_factor', ascending=False)[:3])\n",
    "    \n",
    "    return significance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_id_map():\n",
    "    \n",
    "    with open('./test-collection/cacm.query.txt') as f:\n",
    "        queries = f.read()\n",
    "    \n",
    "    query_ids = re.findall(r'<DOCNO> \\d+ </DOCNO>', queries)\n",
    "    query_ids = [re.findall(r'\\d+', q)[0] for q in query_ids]\n",
    "    \n",
    "    queries = re.split(r'</DOC>', queries)\n",
    "    queries = [l.replace('</DOCNO>', '').replace('\\n', ' ')\\\n",
    "               .replace('</DOC>', '').replace('<DOC>', '')\\\n",
    "               .replace('<DOCNO>', '') for l in queries]\n",
    "    \n",
    "    #queries = [re.sub(r'^\\d*\\s\\s', '',l) for l in queries]\n",
    "\n",
    "    queries = [re.sub(r'\\s{2,5}', '',l) for l in queries]\n",
    "    queries = [re.sub(r'\\d{1,2}', '',l) for l in queries]\n",
    "    \n",
    "    queries  = pd.DataFrame({'query_ids': query_ids, \n",
    "                             'queries': queries[:-1]}).set_index('query_ids')\n",
    "    \n",
    "    return query_ids,queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids,queries = get_query_id_map()\n",
    "queries.to_csv('queries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def iterate_queries(queries, path = './task1-JMQL/'):\n",
    "    \n",
    "    queries_ids = [f.split('.')[0].replace('Q', '') for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    for query_id in tqdm(queries_ids):\n",
    "\n",
    "        query_results = pd.read_csv(path + 'Q' + query_id + '.txt', sep='\\t', names = ['qid', 'dumb_thing', 'doc_id',\n",
    "                                                                                         'rank','score', 'system'])\n",
    "        \n",
    "        get_text_summary(list(query_results['doc_id']),\\\n",
    "                         queries.loc[queries.index == query_id, 'queries'][query_id])[['result', 'sentence']]\\\n",
    "                         .to_html('./snippets/' + query_id + '.html',\n",
    "                                  index = False, escape = False, border = 0, max_rows = None, max_cols = None)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489324a4193b4433b1ab7f25204835eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iterate_queries(queries, path = './task1-JMQL/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "by providing efficient interprocess communication primitives.  cacm february, 1979  cheriton, d. malcolm, m. melen, l. sager, g.  portability, real time, operating systems, minicomputer  3.80 4.30 4.35  ca790206 dh april 12, 1979  9:10 am  2319 4 3127 2378 4 3127 2320 4 3127 2632 4 3127 2738 4 3127 2740 4 3127 2868 4 3127 2928 4 3127 3127 4 3127 3127 4 3127 2080 5 3127 2277 5 3127 3127 5 3127 3127 5 3127 3127 5 3127    \n",
      "    thoth, a portable real-time operating system\n",
      " thoth isa real-time operating system which is designed to be portable over a large set of machines\n",
      "\n",
      "\n",
      "chairman  cacm june, 1961  bright, h. s.  ca610603 jb march 16, 1978  10:55 pm  322 5 322 322 5 322 322 5 322    \n",
      "the problem of operating a computer efficiently in view of the growing number of programming systems\n",
      "incompatibilities are currently resolved by manually setting up the computer for each system as required\n",
      "\n",
      "\n",
      "of the system is with an interpretive translator on an ibm 1620 computer.  cacm july, 1964  hellerman, h.  ca640714 jb march 9, 1978  8:02 pm  1033 5 1033 1033 5 1033 1033 5 1033    \n",
      "the completely symbolic operating system includes display and entry of\n",
      "    experimental personalized array translator system  a system designed for intimate man-machine interaction in a general-purpose problem-solving\n"
     ]
    }
   ],
   "source": [
    "def write_snippets(significance_df, query):\n",
    "    with open('snippets/' + query + '.results', 'w') as f:   \n",
    "        for result in significance_df['result'].unique():\n",
    "            for index, sentence in significance_df.loc[significance_df['result'] == result].iterrows():\n",
    "                f.write(sentence['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
