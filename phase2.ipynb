{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import operator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def casefolding(data):\n",
    "    return data.lower()\n",
    "\n",
    "def punctuationHandling(data):\n",
    "    regex = r\"(?<!\\d)[.,;:*!\\\"\\'#$%&()+/<=>?@[\\]_^`~{|}∑α](?!\\d)\"\n",
    "    data = re.sub(regex, \"\", data, 0)\n",
    "    data = re.sub(r'\\d+', '', data)\n",
    "    regex = r\"[;:*!\\\"\\'#$%&()+/<=>?@[\\]_^`~{|}]\"\n",
    "    data = re.sub(regex, \"\", data, 0)\n",
    "#     data = data.strip(string.punctuation)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_sentences(result):\n",
    "    doc   = open(\"test-collection/cacm/\"+ result + \".html\",\"r\")\n",
    "    data = doc.read()\n",
    "    data = BeautifulSoup(data, \"lxml\").text\n",
    "    data = casefolding(data)\n",
    "    data = re.sub(\"\\d+\",\"\",data)\n",
    "    data = re.sub(\"\\n\",\".\\n\",data)\n",
    "    sentences  = re.split(r\"\\.[\\s\\n]+\",data)\n",
    "    sentences = [s.replace(\"\\n\",\" \").replace(\"\\t\",\" \") for s in sentences]\n",
    "    sentences = [re.sub(' +',' ',s) for s in sentences]\n",
    "    sentences = [punctuationHandling(s) for s in sentences]\n",
    "    sentences = [x for x in sentences if x]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def getDocWordFreq(word,document):\n",
    "    value = 0\n",
    "    if unigram_invertedlist_count.get(word):\n",
    "        for val in unigram_invertedlist_count[word][1]:\n",
    "            if(val[0]==document):\n",
    "                value = val[1]\n",
    "    return value\n",
    "\n",
    "def calculate_significant_words(sentences, document, \\\n",
    "                                query_words, inverted_index):\n",
    "    \n",
    "    significant_words = []\n",
    "    words             = []\n",
    "    sd                = len(sentences)\n",
    "    \n",
    "    if (sd < 25): thresh = 4 - 0.1 * (25 - sd)\n",
    "    \n",
    "    elif ((25 <= sd) and (sd <= 40)): thresh = 4\n",
    "    \n",
    "    else: thresh = 4 + 0.1 * (sd - 40)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words.extend(word_tokenize(sentence))\n",
    "    \n",
    "    for word in words:   \n",
    "        f_dw = getDocWordFreq(word, document)\n",
    "        if f_dw >= thresh: significant_words.append(word)\n",
    "            \n",
    "    significant_words.extend(query_words)\n",
    "    significant_words = list(set(significant_words))\n",
    "\n",
    "\n",
    "    return significant_words\n",
    "\n",
    "\n",
    "def calculate_significance_factor(sentences, significant_words):\n",
    "    \n",
    "    word_count = 0\n",
    "    significance_factor = {}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence          = word_tokenize(sentence)\n",
    "        filtered_significance_words = set(tokenized_sentence)\\\n",
    "                                        .intersection(set(significant_words))\n",
    "        min_index = 100000;\n",
    "        max_index = -1;\n",
    "        \n",
    "        filtered_significance_words = [x for x in filtered_significance_words if x not in commonwords]\n",
    "    \n",
    "        for token in tokenized_sentence:\n",
    "            #print(\"token\",token)\n",
    "            if token in filtered_significance_words: \n",
    "                \n",
    "                new_max_index = max([i for i, x in enumerate(tokenized_sentence) if x == token])\n",
    "                new_min_index = min([i for i, x in enumerate(tokenized_sentence) if x == token])\n",
    "                \n",
    "                if(min_index > new_min_index):\n",
    "                    min_index = new_min_index\n",
    "                \n",
    "                if(max_index < new_max_index):\n",
    "                    max_index = new_max_index\n",
    "                \n",
    "                text_span = (max_index - min_index) + 0.0001\n",
    "        \n",
    "                count = sum([1 for w in tokenized_sentence[min_index: max_index] if w in filtered_significance_words])\n",
    "                significance_factor[sentence] = count**2/text_span\n",
    "                \n",
    "    significance_factor = dict(sorted(significance_factor.items(), key=operator.itemgetter(1), reverse=True)[:3])\n",
    "    return significance_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_summary(results, query):\n",
    "    query_words     = query.split(' ')\n",
    "    query_words     = [x for x in query_words if x not in commonwords]\n",
    "    significance_df = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    for result in results:\n",
    "        significance_dict = []\n",
    "        sentences         = get_sentences(result)\n",
    "        significant_words = calculate_significant_words(sentences, \n",
    "                                                        result, query_words, \n",
    "                                                        inverted_index)\n",
    "        \n",
    "        for sent, score in calculate_significance_factor(sentences, significant_words).items():\n",
    "            \n",
    "            for query_word in query_words:\n",
    "                \n",
    "                sent = sent.replace(query_word, '<b> ' + query_word + '</b>')\n",
    "        \n",
    "            significance_dict.append({'result' : result, \n",
    "                                        'significance_factor': score,\n",
    "                                         'sentence': sent})        \n",
    "        significance_df = significance_df.append(pd.DataFrame.from_dict(significance_dict))\n",
    "    return significance_df\n",
    "\n",
    "def get_query_id_map():\n",
    "    \n",
    "    with open('./test-collection/cacm.query.txt') as f:\n",
    "        queries = f.read()\n",
    "    \n",
    "    query_ids = re.findall(r'<DOCNO> \\d+ </DOCNO>', queries)\n",
    "    query_ids = [re.findall(r'\\d+', q)[0] for q in query_ids]\n",
    "    \n",
    "    queries = re.split(r'</DOC>', queries)\n",
    "    queries = [l.replace('</DOCNO>', '').replace('\\n', ' ')\\\n",
    "               .replace('</DOC>', '').replace('<DOC>', '')\\\n",
    "               .replace('<DOCNO>', '') for l in queries]\n",
    "    \n",
    "    #queries = [re.sub(r'^\\d*\\s\\s', '',l) for l in queries]\n",
    "\n",
    "    queries = [re.sub(r'\\s{2,5}', '',l) for l in queries]\n",
    "    queries = [re.sub(r'\\d{1,2}', '',l) for l in queries]\n",
    "    \n",
    "    queries  = pd.DataFrame({'query_ids': query_ids, \n",
    "                             'queries': queries[:-1]}).set_index('query_ids')\n",
    "    \n",
    "    return query_ids,queries\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def iterate_queries(queries, path = 'baseline-runs/task1-bm25'):\n",
    "    \n",
    "    queries_ids = [f.split('.')[0].replace('Q', '') for f in listdir(path) if isfile(join(path, f))]\n",
    "    queries_ids = [x for x in queries_ids if x]\n",
    "    \n",
    "    print(len(queries_ids))\n",
    "    for query_id in tqdm(queries_ids):\n",
    "#         print(\"Queryid\",query_id)\n",
    "        query_results = pd.read_csv(path + 'Q' + query_id + '.txt', sep='\\t', names = ['qid', 'Q0', 'doc_id',\n",
    "                                                                                         'rank','score', 'system'])\n",
    "       \n",
    "        get_text_summary(list(query_results['doc_id']),\\\n",
    "                         queries.loc[queries.index == query_id, 'queries'][query_id])[['result', 'sentence']]\\\n",
    "                         .to_html('phase2-output/snippets/' + query_id + '.html',\n",
    "                                  index = False, escape = False, border = 0, max_rows = None, max_cols = None)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# significance_factor = calculate_significance_factor(sentences, significant_words)\n",
    "unigram_invertedlist_count = {}\n",
    "inverted_index = {} \n",
    "commonwords = []\n",
    "query_ids,queries = get_query_id_map()\n",
    "queries.to_csv('queries.csv')\n",
    "with open ('reusable_data/unigram_invertedlist_count.pkl', 'rb') as f: \n",
    "    unigram_invertedlist_count = pickle.load(f)\n",
    "    \n",
    "with open('reusable_data/inverted_index.pkl', 'rb') as f:\n",
    "    inverted_index = pickle.load(f)\n",
    "commonwords     = open('test-collection/common_words', \"r\")\n",
    "commonwords     = commonwords.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57705c77b744bf9bd556275a09fea14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# significance_factor = calculate_significance_factor(sentences, significant_words)\n",
    "iterate_queries(queries, path = 'baseline-runs/task1-bm25/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
