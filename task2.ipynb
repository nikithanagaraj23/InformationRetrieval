{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from collections import Counter,OrderedDict\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import glob\n",
    "import pickle\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inverted_unigram_dict = dict()\n",
    "unigram_termcount = {}\n",
    "unigram_corpus_count = 0\n",
    "path = 'corpus'\n",
    "smoothening_factor = 0.35\n",
    "\n",
    "\n",
    "def casefolding(data):\n",
    "    return data.lower()\n",
    "\n",
    "def punctuationHandling(data):\n",
    "    regex = r\"(?<!\\d)[.,;:*!\\\"\\'#$%&()+/<=>?@[\\]_^`~{|}∑α](?!\\d)\"\n",
    "    data = re.sub(regex, \"\", data, 0)\n",
    "    data = re.sub(r'\\d+', '', data)\n",
    "    regex = r\"[;:*!\\\"\\'#$%&()+/<=>?@[\\]_^`~{|}]\"\n",
    "    data = re.sub(regex, \"\", data, 0)\n",
    "#     data = data.strip(string.punctuation)\n",
    "    return data\n",
    "\n",
    "def removeWhitespace(data):\n",
    "    data = ' '.join(data.split())\n",
    "    return data\n",
    "\n",
    "def createCorpusFile(heading,maincontent):\n",
    "    fo = open(\"corpus/\"+str(heading)+\".txt\", \"w\")\n",
    "    fo.write(maincontent)\n",
    "    fo.close()\n",
    "\n",
    "def parseDocs():\n",
    "    for filename in glob.glob(\"test-collection/cacm/*.html\"):\n",
    "        fo = open(filename, \"r\")\n",
    "        heading = os.path.basename(filename).split(\".\")[0]\n",
    "        data = fo.read()\n",
    "        maincontent = BeautifulSoup(data, \"lxml\").text\n",
    "        maincontent = casefolding(maincontent)\n",
    "        maincontent = punctuationHandling(maincontent)\n",
    "        maincontent = removeWhitespace(maincontent)\n",
    "        createCorpusFile(heading,maincontent)\n",
    "        \n",
    "def createIndexDict(file, ngram_dict, inverted_dict):\n",
    "    for key,value in ngram_dict.items():\n",
    "        if (inverted_dict.get(key)):\n",
    "            inverted_dict.get(key).append((os.path.basename(file).split(\".\")[0],value))\n",
    "        else:\n",
    "            inverted_dict[key] = [(os.path.basename(file).split(\".\")[0],value)]\n",
    "    return inverted_dict\n",
    "\n",
    "def getInvertedListCount(index_list):\n",
    "    invertedlist_count = {}\n",
    "    for key,val in index_list.items():\n",
    "        invertedlist_count[key] = [len(val),val]\n",
    "    return invertedlist_count\n",
    "\n",
    "def getDocWordFreq(word,document):\n",
    "    value = 0\n",
    "    if unigram_invertedlist_count.get(word):\n",
    "        for val in unigram_invertedlist_count[word][1]:\n",
    "            if(val[0]==document):\n",
    "                value = val[1]\n",
    "    return value\n",
    "\n",
    "def getCorpusWordFreq(word):\n",
    "    if unigram_invertedlist_count.get(word):\n",
    "        return unigram_invertedlist_count[word][0]\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def query_preprocessor(filepath = 'test-collection/cacm.query.txt'):\n",
    "    with open(filepath) as f: queries = f.read()\n",
    "    queries = [l.replace('</DOCNO>', '').replace('\\n', ' ').replace('</DOC>', '').replace('<DOC>', '')[1:] for l in queries.split('<DOCNO>')]\n",
    "    queries = [re.sub(r'^\\d*\\s\\s', '',l) for l in queries]\n",
    "    queries = [s.lower() for s in queries]\n",
    "    queries = [punctuationHandling(query) for query in queries]\n",
    "    queries = [removeWhitespace(query) for query in queries]\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bm25(document,query,document_bm25_score_dict):\n",
    "    total_score = 0\n",
    "    score = 0\n",
    "    query_list = query.split(\" \")\n",
    "    \n",
    "    average_length = unigram_corpus_count / number_of_docs\n",
    "    for query_word in query_list: \n",
    "        R = 0.0\n",
    "        r = 0.0\n",
    "        #number of docs containing the term\n",
    "        \n",
    "        n = unigram_invertedlist_count.get(query_word)[0] if unigram_invertedlist_count.get(query_word) else 0\n",
    "#         n = unigram_invertedlist_count.get(query_word)[0]\n",
    "        \n",
    "        \n",
    "        #Total number of documents\n",
    "        N = number_of_docs\n",
    "        k1 = 1.2\n",
    "        k2 = 100\n",
    "        #freq of the word in query\n",
    "        qf = query_list.count(query_word)\n",
    "        \n",
    "        #freq of word in the doc\n",
    "        f = getDocWordFreq(query_word,document)\n",
    "        \n",
    "        #to be calculated using b= 0.75\n",
    "        b = 0.75\n",
    "        K = k1*((1-b) + b*(float(unigram_termcount[document])/float(average_length)))\n",
    "        smoothening = 0.5\n",
    "        \n",
    "        first_part = math.log(((r + 0.5)/(R - r + 0.5))/((n - r + 0.5)/(N - n - R + r + 0.5)))\n",
    "        second_part = ((k1 + 1)*f)/(K + f)\n",
    "        third_part = ((k2 + 1) * qf)/(k2 + qf)\n",
    "        score = first_part*second_part*third_part\n",
    "        \n",
    "        total_score += score\n",
    "    document_bm25_score_dict[document] = total_score\n",
    "    return document_bm25_score_dict\n",
    "\n",
    "def populate_bm25(queryString):\n",
    "    document_bm25_score_dict = {}\n",
    "    docList = []\n",
    "    query_list = queryString.split(\" \")\n",
    "    \n",
    "    for query in query_list:\n",
    "        if unigram_invertedlist_count.get(query):\n",
    "            query_doclist = unigram_invertedlist_count[query][1]\n",
    "            for i in query_doclist:\n",
    "                docList.append(i[0])\n",
    "        else:\n",
    "            query_doclist = []\n",
    "            \n",
    "    for docid in docList:\n",
    "        document_bm25_score_dict = get_bm25(docid,queryString,document_bm25_score_dict)\n",
    "        document_bm25_score_dict = dict(sorted(document_bm25_score_dict.items(), key=operator.itemgetter(1), reverse=True)[:100])\n",
    "    return document_bm25_score_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pseudo_relavance(query,bm25_score_dict):\n",
    "    k = 10\n",
    "    #Generate query vector\n",
    "    query_vector = {}\n",
    "    query_list = query.split(\" \")\n",
    "    \n",
    "    for query_word in query_list:\n",
    "        if(query_vector.get(query_word)):\n",
    "            query_vector[query_word] += 1\n",
    "        else:\n",
    "            query_vector[query_word] = 1\n",
    "            \n",
    "    for key,val in unigram_invertedlist_count.items():\n",
    "        if not query_vector.get(key):\n",
    "            query_vector[key] = 0\n",
    "    relevance_vector,relevance_vector_magnitude = generate_relevance_vector(query,bm25_score_dict,k)\n",
    "    \n",
    "    non_relevance_vector,non_relevance_vector_magnitude = generate_non_relevance_vector(query,bm25_score_dict,k)\n",
    "\n",
    "    expanded_q = query_expansion(query,query_vector,relevance_vector,relevance_vector_magnitude,\n",
    "                    non_relevance_vector,non_relevance_vector_magnitude)\n",
    "   \n",
    "    return(expanded_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_relevance_vector(query,bm25_score_dict,k):\n",
    "    sorted_doc = dict(sorted(bm25_score_dict.items(), key=operator.itemgetter(1), reverse=True)[:k])\n",
    "    relevance_vector = {}\n",
    "    for key,val in sorted_doc.items():\n",
    "        with open ('corpus/'+key+'.txt', 'r') as f: \n",
    "            doc_list = f.read().split()\n",
    "            for term in doc_list:\n",
    "                if(relevance_vector.get(term)):\n",
    "                    relevance_vector[term] += 1\n",
    "                else:\n",
    "                    relevance_vector[term] = 1\n",
    "\n",
    "            for token,val in unigram_invertedlist_count.items():\n",
    "                if not relevance_vector.get(token):\n",
    "                        relevance_vector[token] = 0\n",
    "                        \n",
    "    relevance_vector_magnitude = 0\n",
    "    for key,val in relevance_vector.items():\n",
    "        relevance_vector_magnitude += float(val**2)\n",
    "    \n",
    "    relevance_vector_magnitude = math.sqrt(relevance_vector_magnitude)\n",
    "    return(relevance_vector,relevance_vector_magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_non_relevance_vector(query,bm25_score_dict,k):\n",
    "    sorted_doc = dict(sorted(bm25_score_dict.items(), key=operator.itemgetter(1), reverse=True)[k:])\n",
    "    non_relevance_vector = {}\n",
    "    for key,val in sorted_doc.items():\n",
    "        with open ('corpus/'+key+'.txt', 'r') as f: \n",
    "            doc_list = f.read().split()\n",
    "            for term in doc_list:\n",
    "                if(non_relevance_vector.get(term)):\n",
    "                    non_relevance_vector[term] += 1\n",
    "                else:\n",
    "                    non_relevance_vector[term] = 1\n",
    "\n",
    "            for token,val in unigram_invertedlist_count.items():\n",
    "                if not non_relevance_vector.get(token):\n",
    "                        non_relevance_vector[token] = 0\n",
    "                        \n",
    "    non_relevance_vector_magnitude = 0\n",
    "    for key,val in non_relevance_vector.items():\n",
    "        non_relevance_vector_magnitude += float(val**2)\n",
    "    \n",
    "    non_relevance_vector_magnitude = math.sqrt(non_relevance_vector_magnitude)\n",
    "    return(non_relevance_vector,non_relevance_vector_magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_expansion(query,query_vector,relevance_vector,relevance_vector_magnitude,\\\n",
    "                    non_relevance_vector,non_relevance_vector_magnitude):\n",
    "    query_expansion_dict = {}\n",
    "    for term,val in unigram_invertedlist_count.items():\n",
    "        query_expansion_dict[term] = query_vector[term]+ (0.5/relevance_vector_magnitude) * relevance_vector[term] -\\\n",
    "        (0.15/non_relevance_vector_magnitude) * non_relevance_vector[term]\n",
    "    \n",
    "    query_expansion_dict = dict(sorted(query_expansion_dict.items(), key=operator.itemgetter(1), reverse=True))\n",
    "    \n",
    "    expanded_query = query\n",
    "    no_extra_query_terms = 15\n",
    "    for i in range(no_extra_query_terms):\n",
    "        term =  list(query_expansion_dict.keys())[i]\n",
    "        if term not in query:\n",
    "            expanded_query+= \" \"+term\n",
    "    return expanded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_new_queries():\n",
    "    for old_query in range(len(all_queries)):\n",
    "        expanded_q = pseudo_relavance(all_queries[old_query],bm25_score_dict)\n",
    "        expanded_query_list.append(expanded_q)\n",
    "        \n",
    "    return expanded_query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeToFile(queryid,queryname,lmscore_dict,folder_name,system_name):\n",
    "    fo = open(\"baseline-runs/\"+folder_name+\"/\"+ \"Q\" + str(queryid) +\".txt\", \"w\")\n",
    "    for key,val in lmscore_dict.items():\n",
    "        rank = list(lmscore_dict.keys()).index(key)+1\n",
    "#         print(queryid,\"\\tQ0\\t\",key,\"\\t\",rank,\"\\t\",val,\"LMDirichlet\\n\")\n",
    "        fo.write(str(queryid)+\"\\tQ0\\t\"+str(key)+\"\\t\"+str(rank)+\"\\t\"+str(val)+\"\\t\"+ system_name +\"\\n\")\n",
    "    fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what articles exist which deal with tss time sharing system an operating system for ibm computers\n",
      "i am interested in articles written either by prieve or udo pooch prieve b pooch u\n",
      "intermediate languages used in construction of multi-targeted compilers tcoll\n",
      "im interested in mechanisms for communicating between disjoint processes possibly but not exclusively in a distributed environment i would rather see descriptions of complete mechanisms with or without implementations as opposed to theoretical work on the abstract problem remote procedure calls and message-passing are examples of my interests\n",
      "id like papers on design and implementation of editing interfaces window-managers command interpreters etc the essential issues are human interface design with views on improvements to user efficiency effectiveness and satisfaction\n",
      "interested in articles on robotics motion planning particularly the geometric and combinatorial aspects we are not interested in the dynamics of arm motion\n",
      "i am interested in distributed algorithms - concurrent programs in which processes communicate and synchronize by using message passing areas of particular interest include fault-tolerance and techniques for understanding the correctness of these algorithms\n",
      "addressing schemes for resources in networks resource addressing in network operating systems\n",
      "security considerations in local networks network operating systems and distributed systems\n",
      "parallel languages languages for parallel computation\n",
      "setl very high level languages\n",
      "portable operating systems\n",
      "code optimization for space efficiency\n",
      "find all discussions of optimal implementations of sort algorithms for database management applications\n",
      "find all discussions of horizontal microcode optimization with special emphasis on optimization of loops and global optimization\n",
      "find all descriptions of file handling in operating systems based on multiple processes and message passing\n",
      "optimization of intermediate and machine code\n",
      "languages and compilers for parallel processors especially highly horizontal microcoded machines code compaction\n",
      "parallel algorithms\n",
      "graph theoretic algorithms applicable to sparse matrices\n",
      "computational complexity intractability class-complete reductions algorithms and efficiency\n",
      "i am interested in hidden-line and hidden-surface algorithms for cylinders toroids spheres and cones this is a rather specialized topic in computer graphics\n",
      "distributed computing structures and algorithms\n",
      "applied stochastic processes\n",
      "performance evaluation and modelling of computer systems\n",
      "concurrency control mechanisms in operating systems\n",
      "memory management aspects of operating systems\n",
      "any information on packet radio networks of particular interest are algorithms for packet routing and for dealing with changes in network topography i am not interested in the hardware used in the network\n",
      "number-theoretic algorithms especially involving prime number series sieves and chinese remainder theorem\n",
      "articles on text formatting systems including what you see is what you get systems examples tnroff scribe bravo\n",
      "id like to find articles describing the use of singular value decomposition in digital image processing applications include finding approximations to the original image and restoring images that are subject to noise an article on the subject is h andrews and c patterson outer product expansions and their uses in digital image processing american mathematical andrews h patterson c\n",
      "id like to find articles describing graph algorithms that are based on the eigenvalue decomposition or singular value decomposition of the ajacency matrix for the graph im especially interested in any heuristic algorithms for graph coloring and graph isomorphism using this method\n",
      "articles about the sensitivity of the eigenvalue decomposition of real matrices in particular zero-one matrices im especially interested in the separation of eigenspaces corresponding to distinct eigenvalues articles on the subject c davis and w kahn the rotation of eigenvectors by a permutation siam j numerical analysis vol , no g stewart error bounds for approximate invariant subspaces of closed linear operators siam j numerical analysis vol , no davis c kahn w stewart g\n",
      "currently interested in isolation of root of polynomial there is an old more recent material heindel l\n",
      "probabilistic algorithms especially those dealing with algebraic and symbolic manipulation some examples rabiin probabilistic algorithm on finite field siam waztch probabilistic testing of polynomial identities siam rabinm\n",
      "fast algorithm for context-free language recognition or parsing\n",
      "articles describing the relationship between data types and concurrency eg what is the type of a process when is a synchronization attempt between two processes type correct in a message-passing system is there any notion of the types of messages--ie any way to check that the sender of the message and the receiver are both treating the bit stream as some particular type\n",
      "what is the type of a module i dont want the entire literature on abstract data types here but im not sure how to phrase this to avoid it im interested in questions about how one can check that a module matches contexts in which it is used\n",
      "what does type compatibility mean in languages that allow programmer defined types you might want to restrict this to extensible languages that allow definition of abstract data types or programmer-supplied definitions of operators like\n",
      "list all articles dealing with data types in the following languages that are referenced frequently in papers on the above languages eg catch any languages with interesting type structures that i might have missed\n",
      "theory of distributed systems and databases subtopics of special interest include reliability and fault-tolerance in distributed systems atomicity distributed transactions synchronization algorithms resource allocation lower bounds and models for asynchronous parallel systems also theory of communicating processes and protocols p box yale station new haven conn\n",
      "computer performance evaluation techniques using pattern recognition and clustering la\n",
      "analysis and perception of shape by humans and computers shape descriptions shape recognition by computer two-dimensional shapes measures of circularity shape matching\n",
      "texture analysis by computer digitized texture analysis texture synthesis perception of texture\n",
      "the use of operations research models to optimize information system performance this includes fine tuning decisions such as secondary index selection file reorganization and distributed databases\n",
      "the application of fuzzy subset theory to clustering and information retrieval problems this includes performance evaluation and automatic indexing considerations\n",
      "the use of bayesian decision models to optimize information retrieval system performance this includes stopping rules to determine when a user should cease scanning the output of a retrieval search\n",
      "the use of computer science principles eg data structures numerical methods in generating optimization eg linear programming algorithms this includes issues of the khachian russian ellipsoidal algorithm and complexity of such algorithms\n",
      "the role of information retrieval in knowledge based systems ie expert systems\n",
      "parallel processors in information retrieval\n",
      "parallel processors and paging algorithms\n",
      "modelling and simulation in agricultural ecosystems\n",
      "mathematical induction group theory integers modulo m probability binomial coefficients binomial theorem homomorphism morphism transitivity relations relation matrix syracuse university link hall syracuse n\n",
      "semantics of programming languages including abstract specifications of data types denotational semantics and proofs of correctness hoare a dijkstra e university of massachusetts amherst ma\n",
      "anything dealing with star height of regular languages or regular expressions or regular events\n",
      "articles relation the algebraic theory of semigroups and monoids to the study of automata and regular languages\n",
      "abstracts of articles j backus can programming be liberated from the von neumann style a functional style and its algebra of programs cacm re millo r lipton a perlis letter to acm forum cacm - backus j de millo r lipton r perlis a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithms or statistical packages for anova regression using least squares or generalized linear models system design capabilities statistical formula are of interest students t test wilcoxon and sign tests multivariate and univariate components can be included\n",
      "dictionary construction and accessing methods for fast retrieval of words or lexical items or morphologically related information hashing or indexing methods are usually applied to english spelling or natural language problems\n",
      "hardware and software relating to database management systems database packages back end computers special associative hardware with microcomputers attached to disk heads or things like rap relational or network codasyl or hierarchical models systems like system r ims adabas total etc\n",
      "information retrieval articles by gerard salton or others about clustering bibliographic coupling use of citations or co-citations the vector space model boolean search methods using inverted files feedback etc salton g\n",
      "results relating parallel complexity theory both for prams and uniform circuits\n",
      "algorithms for parallel computation and especially comparisons between parallel and sequential algorithms\n",
      "list all articles on el and ecl el may be given as el i dont remember how they did it\n"
     ]
    }
   ],
   "source": [
    "expanded_query_list = []\n",
    "for filename in glob.glob(\"corpus/*.txt\"):\n",
    "    fo = open(filename, \"r\")\n",
    "    data = fo.read()\n",
    "    tokens = nltk.word_tokenize(data)\n",
    "    unigramlist = nltk.word_tokenize(data)\n",
    "    unigram_termcount[os.path.basename(filename).split(\".\")[0]] = len(unigramlist)\n",
    "    unigram_corpus_count = unigram_corpus_count + len(unigramlist)\n",
    "    unigram_dict = Counter(unigramlist)\n",
    "    inverted_unigram_dict = createIndexDict(filename, unigram_dict, inverted_unigram_dict)\n",
    "    fo.close()\n",
    "    \n",
    "unigram_invertedlist_count = getInvertedListCount(inverted_unigram_dict)\n",
    "number_of_docs = len(glob.glob('corpus/*.txt'))\n",
    "\n",
    "all_queries = query_preprocessor()[1:]\n",
    "\n",
    "for i in range(len(all_queries)):\n",
    "    print(all_queries[i])\n",
    "    bm25_score_dict = populate_bm25(all_queries[i])\n",
    "#     writeToFile(i+1,all_queries[i],bm25_score_dict,\"task1-bm25\",\"ccisneu_wordunigram_BM25\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-27ecf6752464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbm25_score_dict_expanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulate_bm25\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_expanded_queries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mwriteToFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_queries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbm25_score_dict_expanded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"task2-bm25-pseudo-relevance\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ccisneu_wordunigram_BM25_PSEUDO_RELEVANCE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "all_expanded_queries = get_all_new_queries()\n",
    "\n",
    "for i in range(len(all_expanded_queries)):\n",
    "#     print(i+1)\n",
    "    bm25_score_dict_expanded = populate_bm25(all_expanded_queries[i])\n",
    "    writeToFile(i+1,all_queries[i],bm25_score_dict_expanded,\"task2-bm25-pseudo-relevance\",\"ccisneu_wordunigram_BM25_PSEUDO_RELEVANCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
